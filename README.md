# ğŸŒ€ Solveur Lorentzien avec Cauchy Loss â€” IRLS + L-BFGS

**Auteur : Thibaut LOMBARD**  
**Licence : MIT**

Ce dÃ©pÃ´t contient un solveur robuste basÃ© sur une approximation **quasi-lorentzienne** de la perte de Cauchy, implÃ©mentÃ© via un schÃ©ma **IRLS â€” Iteratively Reweighted Least Squares**.

Lâ€™objectif est de fournir une alternative robuste aux mÃ©thodes classiques (MSE, L-BFGS, Adam), capable de **rÃ©sister aux outliers** et dâ€™Ã©viter les minima dÃ©gÃ©nÃ©rÃ©s liÃ©s Ã  lâ€™hypothÃ¨se gaussienne des erreurs.



## ğŸš€ Pourquoi un solveur Â« Lorentzien Â» ?

La plupart des optimisations utilisent la perte quadratique :
```
L_MSE = âˆ‘ (y - fÎ¸(x))Â²
```
Elle explose en prÃ©sence dâ€™outliers.

La perte **de Cauchy**, issue dâ€™un modÃ¨le lorentzien, limite lâ€™influence des grandes erreurs :
```
L_Cauchy = âˆ‘ log(1 + rÂ² / ÏƒÂ²)
```
oÃ¹ `r = y âˆ’ fÎ¸(x)` et `Ïƒ` contrÃ´le lâ€™Ã©chelle des rÃ©sidus.



## ğŸ” Approche quasi-lorentzienne via IRLS + L-BFGS

La perte de Cauchy nâ€™est pas quadratique â†’ difficile Ã  minimiser directement.  
On lâ€™approxime itÃ©rativement par une perte quadratique **pondÃ©rÃ©e** :
```
log(1 + rÂ² / ÏƒÂ²) â‰ˆ w(r) Â· rÂ² + constante
```
avec le **poids Lorentzien** :
```
w(r) = 1 / (ÏƒÂ² + rÂ²)
```
â†’ Les points Ã©loignÃ©s (outliers) obtiennent un poids faible  
â†’ Les points fiables guident rÃ©ellement la descente


## ğŸ§® Boucle IRLS

Pour des donnÃ©es `{(xi, yi)}` et un modÃ¨le `Å· = fÎ¸(x)` :

1. Initialiser les paramÃ¨tres `Î¸`
2. Choisir ou estimer `Ïƒ`
3. RÃ©pÃ©ter :
```
ri = yi - fÎ¸(xi) # rÃ©sidus
wi = 1 / (ÏƒÂ² + riÂ²) # poids Lorentziens
Minimiser âˆ‘ wi Â· (yi - fÎ¸(xi))Â² avec L-BFGS
Ïƒ â† median(|ri|) # optionnel : adaptation automatique
```
â†’ Chaque itÃ©ration rÃ©sout un problÃ¨me localement quadratique  
â†’ L-BFGS assure une mise Ã  jour stable et prÃ©cise  
â†’ Les outliers voient leur poids tendre vers zÃ©ro

## ğŸ§  Utilisation avec un rÃ©seau de neurones (PyTorch)

Avec PyTorch, on ne peut pas rÃ©soudre les moindres carrÃ©s pondÃ©rÃ©s analytiquement.
On applique donc une boucle interne dâ€™optimisation lâ€™idÃ©e IRLS reste identique.

```python
import torch

def cauchy_irls_step(model, X, y, sigma=1.0):
    model.eval()
    with torch.no_grad():
        y_pred = model(X)
        r = y - y_pred
        w = 1.0 / (sigma**2 + r**2)

    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    for _ in range(5):  # mini-optim interne
        optimizer.zero_grad()
        y_pred = model(X)
        loss = (w * (y - y_pred)**2).mean()
        loss.backward()
        optimizer.step()
```

## Avantages

* âœ” RÃ©sultats stables
* âœ” ParamÃ¨tres corrects malgrÃ© les outliers
* âœ” Surclasse nettement une rÃ©gression MSE classique


| CritÃ¨re                      |  Lorentzien |
| ---------------------------- | :---------: |
| Sensible aux outliers        |      âŒ      |
| Mise Ã  jour stable           | âœ”ï¸ (L-BFGS) |
| InterprÃ©table (poids)        |      âœ”ï¸     |
| Compatible modÃ¨les complexes |      âœ”ï¸     |
| Adaptation automatique du Ïƒ  |      âœ”ï¸     |

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/cauchy
python solver.py
```
