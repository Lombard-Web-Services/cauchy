# ğŸŒ€ Solveur Lorentzien avec Cauchy Loss â€” IRLS Robust Optimizer

**Auteur : Thibaut LOMBARD**  
**Licence : MIT**

Ce dÃ©pÃ´t contient un solveur robuste basÃ© sur une approximation **quasi-lorentzienne** de la perte de Cauchy, implÃ©mentÃ© via un schÃ©ma **IRLS â€” Iteratively Reweighted Least Squares**.

Lâ€™objectif est de fournir une alternative robuste aux mÃ©thodes classiques (MSE, L-BFGS, Adam), capable de **rÃ©sister aux outliers** et dâ€™Ã©viter les minima dÃ©gÃ©nÃ©rÃ©s liÃ©s Ã  lâ€™hypothÃ¨se gaussienne des erreurs.



## ğŸš€ Pourquoi un solveur Â« Lorentzien Â» ?

La plupart des optimisations utilisent la perte quadratique :
```
L_MSE = âˆ‘ (y - fÎ¸(x))Â²
```
Elle explose en prÃ©sence dâ€™outliers.

La perte **de Cauchy**, issue dâ€™un modÃ¨le lorentzien, limite lâ€™influence des grandes erreurs :
```
L_Cauchy = âˆ‘ log(1 + rÂ² / ÏƒÂ²)
```
oÃ¹ `r = y âˆ’ fÎ¸(x)` et `Ïƒ` contrÃ´le lâ€™Ã©chelle des rÃ©sidus.



## ğŸ” Approche quasi-lorentzienne via IRLS + L-BFGS

La perte de Cauchy nâ€™est pas quadratique â†’ difficile Ã  minimiser directement.  
On lâ€™approxime itÃ©rativement par une perte quadratique **pondÃ©rÃ©e** :
```
log(1 + rÂ² / ÏƒÂ²) â‰ˆ w(r) Â· rÂ² + constante
```
avec le **poids Lorentzien** :
```
w(r) = 1 / (ÏƒÂ² + rÂ²)
```
â†’ Les points Ã©loignÃ©s (outliers) obtiennent un poids faible  
â†’ Les points fiables guident rÃ©ellement la descente


## ğŸ§® Boucle IRLS

Pour des donnÃ©es `{(xi, yi)}` et un modÃ¨le `Å· = fÎ¸(x)` :

1. Initialiser les paramÃ¨tres `Î¸`
2. Choisir ou estimer `Ïƒ`
3. RÃ©pÃ©ter :
```
ri = yi - fÎ¸(xi) # rÃ©sidus
wi = 1 / (ÏƒÂ² + riÂ²) # poids Lorentziens
Minimiser âˆ‘ wi Â· (yi - fÎ¸(xi))Â² avec L-BFGS
Ïƒ â† median(|ri|) # optionnel : adaptation automatique
```
â†’ Chaque itÃ©ration rÃ©sout un problÃ¨me localement quadratique  
â†’ L-BFGS assure une mise Ã  jour stable et prÃ©cise  
â†’ Les outliers voient leur poids tendre vers zÃ©ro



## ğŸ Exemple minimal (fourni dans le dÃ©pÃ´t)

Le solveur ci-dessous effectue une rÃ©gression linÃ©aire robuste :

```python
import numpy as np
from scipy.optimize import minimize

# DonnÃ©es synthÃ©tiques avec outliers
np.random.seed(0)
X = np.linspace(-5, 5, 100).reshape(-1, 1)
y = 2 * X.ravel() + 1 + np.random.normal(0, 0.5, 100)
y[::10] += np.random.normal(0, 10, 10)  # outliers

def irls_cauchy(X, y, max_iter=15, sigma=1.0):
    n = X.shape[1]
    theta = np.random.randn(n + 1)  # [b, w]

    for k in range(max_iter):
        y_pred = X @ theta[1:] + theta[0]
        r = y - y_pred
        w = 1.0 / (sigma**2 + r**2 + 1e-8)

        def weighted_mse(params):
            y_p = X @ params[1:] + params[0]
            return np.sum(w * (y - y_p)**2)

        # Mise Ã  jour par L-BFGS
        theta = minimize(weighted_mse, theta, method='L-BFGS-B').x
        sigma = np.median(np.abs(r)) + 1e-6

    return theta

theta_est = irls_cauchy(X, y)
print("Î¸ estimÃ© (b, w) :", theta_est)```

## ğŸ§  Avantages

âœ” RÃ©sultats stables
âœ” ParamÃ¨tres corrects malgrÃ© les outliers
âœ” Surclasse nettement une rÃ©gression MSE classique
| CritÃ¨re                      |  Lorentzien |
| ---------------------------- | :---------: |
| Sensible aux outliers        |      âŒ      |
| Mise Ã  jour stable           | âœ”ï¸ (L-BFGS) |
| InterprÃ©table (poids)        |      âœ”ï¸     |
| Compatible modÃ¨les complexes |      âœ”ï¸     |
| Adaptation automatique du Ïƒ  |      âœ”ï¸     |

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/cauchy
python solver.py
```
